from typing import Dict, Any, List, Optional, Union
import numpy as np
from pathlib import Path
import asyncio
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredHTMLLoader
)
from config.logger import get_logger
from langchain_community.embeddings import DashScopeEmbeddings

logger = get_logger(__name__)

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨ï¼šæ¸…æ´—ã€åˆ†å—ã€å‘é‡åŒ–"""
    
    def __init__(
        self,
        embedding_model: str = "text-embedding-v4",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        self.embedding_model = DashScopeEmbeddings(model=embedding_model,dashscope_api_key='sk-eed6accea0594ebabe804410af709a80')
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
        # æ–‡ä»¶ç±»å‹åˆ°åŠ è½½å™¨çš„æ˜ å°„
        self._loaders = {
            ".txt": TextLoader,
            ".md": UnstructuredMarkdownLoader, 
            ".pdf": PyPDFLoader,
            ".html": UnstructuredHTMLLoader
        }

    async def process_file(
        self, 
        file_path: Union[str, Path],
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶:åŠ è½½ã€åˆ†å—ã€å‘é‡åŒ–
        è¿”å›å¯ç›´æ¥ä¼ å…¥add_documentçš„æ–‡æ¡£åˆ—è¡¨
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
            
        # 1. åŠ è½½æ–‡æ¡£
        loader_cls = self._loaders.get(file_path.suffix.lower())
        if not loader_cls:
            raise ValueError(f"Unsupported file type: {file_path.suffix}")
            
        loader = loader_cls(str(file_path))
        documents = await asyncio.to_thread(loader.load)
        
        # 2. æ–‡æœ¬åˆ†å—
        chunks = self.text_splitter.split_documents(documents)
        
        # 3. å‘é‡åŒ–(æ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡)
        texts = [chunk.page_content for chunk in chunks]
        embeddings: List[List[float]] = []
        batch_size = 10

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            # è°ƒç”¨é˜»å¡çš„ embed_documents æ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            emb_batch = await asyncio.to_thread(self.embedding_model.embed_documents, batch)

            for item in emb_batch:
                embeddings.append(item)
            
        
        # 4. å‡†å¤‡è¿”å›æ•°æ®
        results = []
        base_metadata = metadata or {}
        
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            chunk_metadata = {
                **base_metadata,
                "source": str(file_path),
                "chunk_index": i,
                **chunk.metadata  # ä¿ç•™åŸæ–‡æ¡£å…ƒæ•°æ®
            }
            
            results.append({
                "id_str": f"{file_path.stem}_{i}",
                "text": chunk.page_content,
                "embedding": embedding,
                "metadata": chunk_metadata
            })
            
        return results

    async def process_directory(
        self,
        dir_path: Union[str, Path],
        recursive: bool = True,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """å¤„ç†æ•´ä¸ªç›®å½•ä¸‹çš„æ–‡æ¡£"""
        dir_path = Path(dir_path)
        if not dir_path.is_dir():
            raise NotADirectoryError(f"Not a directory: {dir_path}")
            
        pattern = "**/*" if recursive else "*"
        all_results = []
        
        for file_path in dir_path.glob(pattern):
            if file_path.suffix.lower() in self._loaders:
                try:
                    results = await self.process_file(file_path, metadata)
                    all_results.extend(results)
                except Exception as e:
                    logger.error(f"Error processing {file_path}: {e}")
                    
        return all_results


if __name__ == "__main__":
    import json
    import os

    async def main():
        # åˆå§‹åŒ–å¤„ç†å™¨
        dp = DocumentProcessor()

        # å‡è®¾æˆ‘ä»¬è¦å¤„ç†çš„ç›®å½•è·¯å¾„
        input_dir = "./docs"   # ä¾‹å¦‚: æ”¾äº†è‹¥å¹² txt/md/pdf/html
        output_path = "./processed_results.json"

        # æ‰§è¡Œå¼‚æ­¥ç›®å½•å¤„ç†
        all_results = await dp.process_directory(input_dir)

        # è¾“å‡ºç»“æœç»Ÿè®¡
        print(f"âœ… å…±å¤„ç† {len(all_results)} ä¸ªæ–‡æ¡£å—")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # å†™å…¥åˆ° JSON æ–‡ä»¶
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ ç»“æœå·²ä¿å­˜è‡³: {output_path}")

    # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
    asyncio.run(main())
