## A Survey of LLM-based Agents: Theories, Technologies, Applications and Suggestions


### 理论基础

早期Agent基于强化学习,认为AI智能体可以基于简单的启发式驱动策略函数行动。然而，基于强化学习的智能体可能面临一些障碍[3]，如训练时间长、采样效率低和学习过程不稳定等。

由于具备卓越的多模态理解与生成能力、无与伦比的知识获取与推理能力，以及大语言模型（LLMs）的灵活性和可扩展性，人工智能Agent将LLMs作为核心大脑，试图实现人类水平的感知、认知和行为[5]。

多模态感知实现自主感知能力,复杂的规划实现自发工作能力,可行的具身化或工具利用实现反应能力,有效的多元化记忆实现交互能力。

- 模态编码器（Modality Encoder）：输入多模态原始数据,编码出其他模态的特征向量。
- 模态连接器（Modality Connector）：将模态数据和文本编码数据融合,实现对齐,输出对齐后的模态特征。
- LLM主干（LLM Backbone）：
- 输出投影器（Output Projector）
- 模态生成器（Modality Generator）

？？？不是一个LLM的主干,像是文生图的workflow

任务目标g、环境e、提示集p及语言模型总体参数Θ后，任务分解可形式化表示为[8]：g₀,g₁,...,gₙ = decp(g,e,p,Θ)

其中"decp"表示分解操作，g₀,g₁,...,gₙ代表子目标。Agent应支持通过CoT实现的单路径和多路径推理能力。

根据内外双重对齐来进行多路径推理的选择：外部对齐需要将人类意图或预期目标转化为基于大语言模型智能体的训练目标，通常采用包含监督微调、奖励建模和策略优化的RLHF方法；内部对齐则要求规划过程确保内部优化目标与智能体训练目标保持一致，具体强调通过安全评估、可解释性验证和人类价值观检验来保障规划的对齐性。

然后就是RAG通过外部知识库增强记忆与知识调用能力。

以及例如API调用、代码解释器乃至具身化工具的工具使用能力。

论文首先回顾了支撑 LLM-based Agent 的几大核心理论：

### 关键技术
论文将 LLM-based Agent 的技术架构归纳为四大模块：

感知（Perception）：处理文本、图像、音频、视频等多模态输入，如 BLIP-2、MiniGPT-4、AudioGPT 等。

规划（Planning）：包括任务分解、单/多路径推理（如 CoT、ToT、GoT）、反思机制（如 Reflexion、CRITIC）等，提升决策合理性。

记忆（Memory）：涵盖交互内记忆、跨交互记忆和外部知识。

- 内部交互记忆指单次交互内的历史信息
- 跨交互记忆指跨越多轮交互积累的长期历史信息
- 或是通过RAG技术获取高质量外部知识

行动（Action）：进行工具理解、工具调用、工具整合的能力。

### 应用与评估

LLM-based Agent 已广泛应用于多个领域：

自然科学：数学（ToRA）、化学（ChemCrow）、生物学（BSDG）；

社会科学：经济学（Alpha-GPT）、法律（LJP-Agent）、心理学（Replika-MWS）；

工程领域：代码生成（GPT-Engineer、AutoGen）、游戏（Voyager、GITM）、工业规划（LLM-Planner）；

评估基准：如 AgentBench（多环境评估）、ToolLLM（工具使用）、SafetyBench（安全性）、AlignBench（中文对齐）等。

### 挑战与建议

作者指出当前 LLM-based Agent 面临的关键挑战，并提出四点建议：

突破内在限制：如幻觉、长上下文处理、多模态推理等；

推动规模化多智能体系统：实现动态调度与高效协作；

强化可控的 AI 对齐：确保遵守法律、伦理与人类价值观；

构建统一综合的评估体系：当前评估分散，亟需标准化平台。

## Survey on evaluation of llm Agent

分为4块。

1. 对agent基本能力的评估（规划和多步推理能力、函数调用和工具使用、自反思、记忆能力）
2. 领域特长能力的评估（web agent、编程Agent、科学推理Agent、对话Agent）
3. Agent在泛用领域的评估
4. 评估Agent 的框架（开发用框架和练习场式环境的框架。）


### 基本能力评估

#### 规划和多步推理能力

大语言模型中的多步推理通常需要执行序列化的逻辑操作（一般涉及3-10个中间步骤）来获得无法通过单步推断得出的解决方案。下列测试用于评价模型在特定领域的推理能力：

- 数学推理：GSM8K,MATH,AQUA-RAT
- 多跳问答：HotpotQA,StrategyQA、MultiRC
- 科学推理：ARC
- 逻辑推理：FOLIO和P-FOLIO
- 约束满足谜题：24点
- 日常尝试：MUSR
- 高难度推理任务:BBH

一些评价规划能力的新基准（2023）：

- ToolEmu:采用基于模拟器的方法评估工具使用型智能体。说明了成功规划需要显式状态追踪和错误恢复能力。
- MINT：针对交互环境中的规划进行评估。发现即使先进的大语言模型也难以应对需要多步骤的长期任务。
- PlanBench：跨领域综合评估框架，表明当前模型擅长短期战术规划但拙于长期战略规划。
- AutoPlanBench：聚焦日常场景的规划评估。证明最前沿的大语言模型智能体仍落后于传统符号规划器。

符号规划任务表现不佳(2024)：

- FlowBench：评估工作流规划能力，重点关注专业知识密集型任务。
- ACPBench：评估大语言模型的核心推理技能。
- NaturalPlan：评估大语言模型如何处理自然语言呈现的现实世界规划任务

这些测试凸显了Agent高效规划所需要的核心能力：

（1）任务分解能力以拆解复杂问题，

（2）状态追踪与信念维护能力以实现精准多步推理，

（3）自我纠错机制以识别并修正错误，

（4）因果理解能力以预测行动结果，

（5）元规划能力以优化决策策略。

#### 函数调用与工具使用

早期研究采用针对性工具，例如通过增强检索能力的语言模型实现检索功能。后续发展则包含更多通用工具，如ToolFormer、Chameleon和MRKL。

一些提供明确参数进行简单交互为内容的评估方法：

- ToolAIpaca 2023
- APIBench 2025
- ToolBench 2023
- BFCL 2024

它们使用合成数据集和基于规则的匹配来建立通过率和结构准确性等基线指标。但在现实应用中有所局限。例如参数可能在多轮对话、对话中未明确提及，或是工具有复杂输入结构和冗长精密输出。

BFCL通过引入组织工具（BFCL v2）和整合多轮多步骤评估逻辑（BFCL v3）来更贴近真实问题复杂性，并强调持续状态管理的重要性。

新的基准测试：

- ToolSandbox：整合有状态工具执行、隐式状态依赖、基于策略的对话评估以及针对任意轨迹中里程碑节点的动态评估策略。
- Seal-Tools：采用自指导方法生成嵌套工具调用，有效建模层级化交互场景。
- API-Bank：通过对话式评估和海量训练数据集强化真实API交互模拟
- ComplexFuncBench：评估隐式参数推理、用户约束遵循及长上下文高效处理等场景。


#### 自我反思

交互式自我反思：

- LLF-Bench
- LLM-Evolve
- Reflection-Bench
- LiveCodeBench：编程基准
- APPS：编程基准

#### 记忆

新的研究引入了记忆机制。如ReadAgent、MemGPT和A-MEM探索了这类方法。

ReadAgent通过内容分组、将事件浓缩为记忆单元并进行段落检索来结构化阅读流程，其有效性已在QUALITY、NarrativeQA和QMSum等数据集上得到验证。类似地，A-MEM提出了一种采用LoCoMo基准评估的高级记忆架构，而MemGPT则采用分层记忆系统，并在NaturalQuestions-Open和多轮对话数据集上进行了测试。

一些新的基准测试：

- LTM-BenchMark
- RAISE
- KARMA

### 特定领域评估

Agent基准测试通过整合三大核心要素，为评估基于大语言模型的Agent提供了系统化框架：首先采用明确定义的任务数据集说明智能体的预期目标；其次构建运行环境（模拟静态/动态场景或真实世界），可集成用户模拟、多样化工具及特定策略；最后运用成功率、效率、精确度等评估指标进行多维度量，支持从单个操作追踪到端到端任务完成的多颗粒度分析。

#### Web Agent

这类AI系统专为网站交互设计，可执行机票预订、商品采购等任务。评估重点包括任务完成效率、网络环境导航能力及安全合规表现。随着Agent进步，评估基准也同步发展，最新成果已能模拟日趋复杂的现实交互场景。

早期研究聚焦基础模拟环境，如MiniWob和MiniWoB++。

后续研究在静态数据集开发上取得突破，支持离线可复现评估。如WebShop。后续的Mind2Web与WebVoyager支持对复杂网站结构导航能力和阶段性目标达成度的综合测评。

近期研究已转向更贴近现实环境的动态在线基准测试。

- WebLinX：提出动态交互模型，要求Agent必须适应网页界面的持续变化，从而检验其决策流程的鲁棒性。
- WebArena/Visual-WebArena:整合了真实用户界面元素与视觉线索，要求Agent不仅能遵循预设流程，还需解读并响应视觉信息。
- ST-WebAgentBench:尝试在静态与动态元素融合的环境中评估网络Agent，揭示其在多变条件下的表现。

#### Code Agent

- SWE-bench基于真实GitHub问题构建，提供端到端评估框架，包括详细的问题描述、完整代码库、执行环境（如Docker）和验证测试。
- SWE-bench Lite聚焦300个涉及缺陷修复的子任务，剔除需复杂多文件修改或无关元素的任务.
- SWE-bench LiteS通过移除存在精确补丁或描述不充分的任务进一步优化数据集.
- SWE-bench Verified仅保留描述清晰且测试用例完备的问题。
- SWE-bench+致力于缓解解决方案泄露和弱测试用例等关键评估缺陷
- SWE-bench Multimodal则针对含可视化元素的JavaScript应用，评估Agent在视觉软件领域的表现。
- SWT-Bench、TDD-Bench Verified专注于评估Agent根据GitHub问题生成测试用例的能力。
- ITBench为评估复杂现实IT自动化任务提供基准。
- SWELancer通过对接自由编程任务，将Agent表现与货币价值挂钩，凸显了在复杂现实场景中实现长期推理与决策的挑战。


#### Science Agent

科学智能体的评估已从早期测试基础推理能力的基准，逐步发展为衡量多样化科研能力的综合框架。

初期基准侧重科学知识记忆与推理能力或是科学文献的整合和情景化分析:

- ARC
- ScienceQA
- ScienceWorld

- QASPER
- QASA
- MS²
- SciRiff进一步拓展评估维度，强调智能体跨科学领域执行用户指令的能力。

当前研究趋势转向开发能加速科研进程的科学智能体，基准已覆盖科研全流程:

1. 科学构思：评估智能体能否自主提出媲美人类专家的创新性研究构想，重点关注科学思维的创造性、相关性与可行性；（Si等，2025）
2. 实验设计：AAAR-1.0数据集等基准测试智能体系统规划实验的能力，包括假设构建、方法选择及符合科学规范的实验流程设计；
3. 实验代码生成：SciCode、ScienceAgentBench、SUPER和CORE-Bench等基准验证智能体生成精准可执行代码的能力，确保代码符合科学协议要求并保持计算准确性；
4. 同行评审生成：测试智能体能否提供超越人类评审质量的深度反馈（Chamoun等，2024）。

整合上列流程的基准：AAAR-1.0、MLGym、DiscoveryWorld、LAB-Bench


#### Chat Agent

略

### 通用能力评估

Agent 正从特定应用场景转向更通用的方向。这些Agent将大语言模型的核心能力与网页浏览、信息检索、代码执行等技能相结合，来解决复杂任务。

- GAIA基准：包含466个人工设计的现实世界问题，用于测试智能体的推理、多模态理解、网页浏览和通用工具使用能力。
- Galileo Agent排行榜：着重评估智能体在数据库查询、在线计算器和网络服务等现实应用中执行函数调用和API调用的能力。
- AgentBench:引入了一套包含操作系统命令、SQL数据库、数字游戏和家务任务的交互环境。

或是聚焦于在全功能计算机操作环境中的表现：

OSWorld、OmniACT和AppWorld等基准测试智能体能否驾驭真实计算机系统、执行复杂任务并协调跨多应用程序的操作。

### Agent 评估框架
略

### 讨论

评估指标正向真实环境和挑战性看齐。

另一方面，新型的方向有

1. 精细化评估推进：缺乏对工具选择、推理质量等中间决策过程的细粒度分析。
2. 成本效率指标：将Token用量、API开销、推理时间等成本效率作为核心指标。
3. 规模化与自动化：采用合成数据生成技术创建多样化任务场景；发展"智能体即评委"的LLM自动化评估。
4. 安全合规性：当前基准对安全性、可信度及政策合规的关注有限。

## Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review

作者对153篇LLM有关的CHI会议的论文集进行研究。专注于LLM在人机交互领域的应用。

### 应用领域

1. 沟通和写作：许多研究将作家视为大型语言模型的目标用户，任务范围从个人日记、电子邮件撰写到故事创作、剧本编写以及一般创意写作。
2. **能力增强**：这一领域包括开发技术以通过改变我们与技术及信息的互动方式来提升人类表现和生产力的论文。
3. 教育：这一领域探索了大型语言模型在提升学生学习体验和改进教育者教学方法方面的潜力。
4. 责任：考虑计算系统对社会、伦理领域的影响，尤其是高风险领域。
5. 编程：聚焦于软件工程和编程领域
6. 语言模型的可靠和有效性：关注于评估和提升llm输出本身
7. 健康：关注于通过llm获取健康相关疾病和病症的管理与预防的知识或是处理健康数据。
8. 设计：用于提供设计有关的帮助，如用户界面设计。
9. 无障碍和老龄化：该领域重点关注残障人士和老年人群体。
10. 创造：该领域涵盖了创意过程及创造力支持工具。

### 应用角色

1. LLM作为系统引擎：在这一角色中，LLM作为系统、原型、算法和编程框架的核心元素发挥作用。
2. LLM作为研究工具：使用LLM执行传统上由研究人员或研究助理完成的任务，包括数据收集、分析或写作。
3. LLM作为参与者或用户：此类研究用大语言模型模拟人类响应和行为。
4. LLM作为研究对象：此类研究探讨大语言模型底层机制与特性的论文，涉及训练数据集、响应输出及问题研究（如幻觉现象）。
5. 用户对LLM的认知：此类包括关于用户如何看待LLMs或LLM驱动工具的研究。

### 不足之处

1. LLM对不同人物存在偏见：大语言模型对不同人群的回复存在差异。
2. LLM训练数据覆盖范围不足：大语言模型的训练数据可能不足或过时。
3. LLM可能输出多种回答：大语言模型的回答具有概率性特征，即使输入相同的提示，其输出仍可能发生不可预测的变化。
4. LLM可能产生幻觉：大语言模型可能产生不准确或完全虚构的信息。
5. LLM的错误或偏差无从查询：由于模型的黑箱性质，大语言模型的输出存在异常时，难以定位出现异常的原因。

6. LLM的计算要求严苛
7. 使用LLM对经济也有要求。
8. LLM缺乏评估标准或度量。

9. 在不同用户和不同情境中，LLM使用过程中的内部效度和外部效度存在差异。
10. 不同的LLM存在效度差异。
11. 提示词也会带来效度差异。

12. LLM可能会带来就业问题。
13. LLM的表示偏差可能会带来认知引导方面的负面效果。
14. LLM的输出可能虚假、带有误导性或是无意义、低质量。
15. LLM可能会用于恶意行为。
16. LLM可能会输出仇恨言论。
17. LLM对电力的消耗引起环境危害。


## Towards AI-Powered Applications: The Development of a Personalised LLM for HRI and HCI

文章使用EEG数据集训练模型,通过外接信息采集设备来捕获人类脑电波数据,从而解析人类情绪,进一步定制化模型。

LLM的个性化作为在线适应(Online Adaption)的子集。

个性化AI模型意味着使模型适应个体需求、偏好和情境，从而显著提升用户参与度、满意度及整体效果[38,39]。个性化体现了模型适应的核心理念——模型通过持续与环境互动，基于观察反馈更新知识体系以契合主要情境[40]。

PLLMs）从持续学习和个性化范式中汲取灵感[50,51]，提供了一种无需全面重训练即可整合新数据集的有效方案。

'PLLMs作为自适应智能体运作，将新数据流分解为离散任务或知识源进行吸收。这种整合丰富了模型的体验知识，使其能对未见场景进行细致推理。与传统重训练方法不同，PLLMs采用特殊设计的提示词动态调整模型行为，确保新数据无缝融入模型推理过程[52,53]。'实际上就是简单的提示词拼接。

智能体可定义为一种通过与所处环境交互来达成目标的应用程序[58]。该智能体可能包含多个模块：一个或多个模型、用于数据采集与决策执行/动作实施的交互模块，以及用于处理采集数据的数据转换与聚合模块。该研究设计的智能体整合了这三个核心模块。

总之,这篇文章主要聚焦于用提示词工程和用户反馈将特定领域数据(如采集到的的脑电波数据)预处理后输入大模型,由LLM判断使用者的情绪状态。

核心内容在基于反馈的改进、持续学习。

## How Do You Want to View This? Generative AI, Creative Ethos, and TTRPG Hobbyists

三个核心问题：

- TTRPG爱好者对生成式AI的"心智模型"是什么？
- 当TTRPG爱好者面临生成式AI进入TTRPG领域时，他们如何应对复杂的伦理问题？
- 如何让TTRPG爱好者在采用生成式实践时，既能符合他们对有效性的定义，又能与创意工作流程实现伦理融合？

思路不对。不在提出“心智模型”->调研->提出对目标用户有益的做法上。

## ThemeViz: Understanding the Effect of Human-AI Collaboration in Theme Development with an LLM-enhanced Interactive Visual System

### 背景

Thematic research(主题研究):研究者围绕一个明确的“主题（theme）”，从不同角度、领域或数据源收集证据，对该主题相关的现象、规律、关系或趋势进行系统分析的研究方法。

关于AI辅助工具的人机交互研究，主要集中在支持主题分析中耗时费力的编码任务（即对文本数据进行片段标注）。这类任务更易于AI高效自动化处理。

相比之下，编码完成后的主题开发任务涉及更复杂的概念化任务(定性任务)更难被ai处理。最新研究表明LLMs有望通过提供研究者可能忽略的替代性解读，在主题开发等诠释性任务中与研究者展开协作[38,49,65]。也就是说,LLMs能生成新颖视角并挑战研究者假设，这正是智力协作的关键特征[25,34,51]。

但是,我们对两个问题认知有限：(1)如何为主题开发等诠释性任务设计有效的人机协作；(2)研究者如何与AI主题开发辅助工具交互。

计算机支持协同工作（CSCW）领域的研究表明，当尊重研究者自主权且AI未经许可不干扰分析时，人机协作在定性数据分析中可能取得成功[23,37]。也就是说定性数据分析的人机协作中给用户自主操作空间十分重要。

除此以外,视觉辅助工具也很重要,直白的对话式工具(如chatGPT)不能很好地支持主题研究。

所以文章通过设计"ThemeViz"来填补这一空白。其提供三个支持：

1. 自主性支持：基于CSCW领域关于研究者自主权重要性的研究成果[23,37]，系统采用"AI辅助"而非"全自动化"模式，通过保留人工编码环节来强化研究者在主题开发全程的主导权；
2. 交互可视化支持：通过动态气泡图呈现主题结构，用户既能纵览全局，又能通过气泡隐喻交互查看相关文本摘录与编码；
3. 提示词工程支持：系统将LLM深度集成，通过后台管理元数据（原始文本/编码/摘录）实现提示词的隐性自动化。

为验证效果，做一做个联合实验室实验与半结构化访谈（28位质性研究者），探究三个核心问题：

RQ1 相比ChatGPT等传统问答界面，ThemeViz对主题开发的助力程度？

RQ2 相较传统界面，ThemeViz设计多大程度促使用户将AI视为主题开发的协作伙伴？

RQ3 ThemeViz中AI辅助的局限性？

其中,重点考察：1）优化图形界面与传统文本问答界面的差异；2）LLM深度融入主题开发等诠释性任务的启示。

研究发现：
1. ThemeViz通过支持多轮迭代与多视角数据观察有效助力主题开发，其提示工程与响应可视化功能广受好评。
2. 但研究者并未将ThemeViz的AI助手（或ChatGPT）视为协作伙伴，归因于AI缺乏自主能动性及双向互动讨论能力。
3. 系统在数据隐私和AI偏见方面也存在局限。


### 研究过程

#### 主题分析

理解人机交互用户研究中的定性数据通常涉及主题分析（TA），其目的是识别、分析和解释数据中的意义模式，即主题[6]。

主题包含6个阶段：

TA包含六个阶段[7, 11]，每个阶段：

1. 熟悉数据
2. 生成初始编码
3. 寻找主题
4. 审查潜在主题
5. 定义和命名主题
6. 撰写报告

本文主要聚焦于3、4、5阶段。因为这些阶段与主题开发直接相关。

支持主题开发具有重要意义，因为这是一个耗时费力且需要大量认知投入的过程——研究者需要对数据进行抽象、综合与诠释来提炼主题与意义[4,7,9,45]。

研究表明，大语言模型(LLMs)为支持主题开发等解释性任务提供了新机遇——通过提供研究者可能未曾考虑的替代性数据解读，LLMs或能协助探索多个主题版本来支持迭代开发过程[38,49,65]。然而目前尚不清楚：质性研究者是否会认可这种人机协作的价值？如何设计LLM增强工具才能在支持高抽象度任务（如主题开发）的同时保持用户自主性[23,37]？

之后说明,一些文献证明了AI生成的主题与人类创建的主题具有一定相似性[32]、也就是证明了大型语言模型在此场景下的适用性。然而有研究仅考察了未经交互系统支持的原始模型应用，且实验环境局限于Jupyter Notebooks等编程平台。这类平台缺乏用户友好的交互设计来简化模型操作流程（尤其对非程序员从业者而言），还要求用户自行设计提示词处理数据收发——这对不熟悉大型语言模型的新手用户颇具挑战性[63]。

因此以下问题尚待明确：1)相较于系统外单独使用大型语言模型，专为质性研究设计的系统能在多大程度上优化主题开发流程；2)此类系统设计具体能带来何种程度的提升。

之后,作者回顾了一些对主题开发的已有的尝试,指出其不足之处如未通过为研究人员提供在使用AI前自行开发主题的机会来支持其自主性以及未能可视化等。

### 设计目标

1. 通过支持手动编码和手动主题开发来保障用户自主权。
2. 通过大语言模型辅助主题建议提升开发效率。
3. 尊重研究者观点的同时,鼓励通过编写提示词进行数据探索。
4. 通过集中式数据管理减轻提示词编写负担。
5. 通过交互式可视化促进理解。
6. 确保AI生成主题基于真实数据。
7. 支持研究者观点驱动的归纳式主题分析。

### 使用示例

通过一个完整的示例流程来说明其达到了各方面的设计目标。并在最后说明了系统的技术架构。

### 研究假设

针对上述研究问题：

1) ThemeViz工具的实用性，2) 用户对AI作为协作伙伴的认知，3) ThemeViz中AI支持的局限性。

提出了一系列假设,并留作后续的调研问题。

之后就是实验、用户体验、访谈等。

最后针对上述3个问题结合受试者反应做完善分析,并认为其最终达到预期结果。


## Interaction Configurations and Prompt Guidance in Conversational AI for Question Answering in Human-AI Teams

这项工作旨在探索如何帮助人类更好地与LLMs交互以生成文本，特别是在问答的背景下。具体而言，作者模拟并测试了在客户支持环境中可能用到的材料，这是现实世界中具有代表性的问答应用场景。

### Introduction

本文测试了两种为人类Agent提供会话式AI辅助的问答配置方案，并与传统纯人工模式（含无AI辅助和有AI辅助两种情形）进行对比。作者探究了哪些因素与回答质量相关并促成有效人机协作，以及何种回答会被视为成功的高质量响应。

所有配置均包含三个基本要素：待回复的问题、含正确答案所需的参考文档、以及基于GPT-4构建的会话式AIAgent（接收问题与参考文档作为输入）。

结果表明人机协作具有潜在价值，但仅限于交互成功的情况。单纯将人类与AI结合并不能自动保证性能提升。
从整体质量看，各条件间答复质量无显著差异；但选取各条件最优的七份答复时，"引导提示"和"对话"条件下的答复显著优于纯AI生成结果（但仍不及纯人工答复）。

分析得出若干关于问答系统人机协作设计的启示：
首先，有必要鼓励使用者在协作过程中更积极发挥作用；
其次，消息建议能改善交互质量——例如"引导提示"条件比"高亮标记"或"对话"条件产生更多成功的AI答复；
第三，引导人类询问"该向AI提什么问题"具有积极作用，这类元提示与成功AI答复数量呈正相关；
最后，设计应确保人机使用相似术语体系，虽然问题重述与成功AI答复正相关，但有时会导致AI失效。

### 研究背景

当人类与AI协作完成工作时，人们期待双方能力的结合能实现超越各自独立工作时的卓越表现与成就。评估这类人机协作时，往往聚焦于绩效指标或人类参与者的主观满意度[10]。

既有研究揭示了影响人机协作质量的多元因素。Inkpen等[23]通过用户与三种模拟算法模型的交互实验发现，用户专业基础与人机优势互补等潜在因素显著影响团队效能——尽管这些模型准确率相同，但真阳性/真阴性率配置各异。Holstein团队[21]的在线实验则探究了明确传达潜在不可观测变量如何影响人们整合模型输出与不可观测因素进行预测。Xu等[58]在视频匿名化任务中分析了AI建议精度与召回率的权衡，比较专业标注员在不同AI辅助模式下的表现。Munyaka等[39]研究发现人机团队成员的决策风格会显著影响协作成效与团队效能感知，并在AI驱动的协作游戏中揭示了AI身份披露的影响。Cabrera团队[8]提出通过展示AI系统在子群体实例中的行为描述来帮助人们合理依赖AI，发现该方式能通过识别AI失误提升人机协作准确率。

现有研究也涉及帮助用户提示LLM的查询建议[28,62]，这类功能已应用于ChatGPT等主流LLM界面。通过深化对LLM对话场景下人机互动的理解，研究者能制定指南并为用户提供恰当引导，最终提升这些AI系统的可用性与效能[3]。

问答系统是自然语言处理（NLP）领域的一项基础性任务[12,45,46]。抽取式问答（或称阅读理解）是最基本的问答形式，模型需要根据给定上下文来回答问题[30,32]。

众多研究者通过分析Stack Overflow和Stack Exchange等问答平台数据来揭示其运行机制。Chua与Banerjee[13]通过Stack Overflow实证研究验证了"答案追寻"框架，重点关注问题可答性。Kabir等人[26]对ChatGPT回复进行了语言学与人性化维度的综合分析。Tian团队[54]则强调了上下文信息在社区问答服务中的关键作用。

当前AI辅助问答研究致力于提升响应质量与速度，其中一个研究方向探索了AI聊天机器人在数字领域促进积极变革与支持提问者的潜力[55]，重点关注用户与虚拟助手互动时的心理反应。在AI辅助客服领域，最新研究涉及客户视角理解、意图识别、采纳因素及体验优化等主题[40]。

本文的目光聚焦于用户如何通过对话Agent解答潜在咨询,以及何种人机写作配置能产生最被期望的相应。

### 设计问题与初步研究

研究者将一些基础基础问题和有助于解决问题的相关文档交给了受试者。由受试者选择不同程度的支持来尝试解决问题。初步研究表明,多数受试者在与Ai对话、参考文档和一个对问题无参考的ai回复、支持文档相关部分高亮和AI Agent对话、和类似问题和回答的不同程度支持中选择了高亮。由此进一步进行了实验设计。

### 实验设计

研究者在初步研究的基础上修改了部分实验内容,并最终指出了研究问题：

1. 哪些配置总体上能促成问答任务中成功的人机协作？
2. 哪些交互配置有助于提升人机团队生成高质量回答的能力？
3. AI辅助问答中优质回答具有哪些特征？

具体的实验不关注

### 实验结果

#### 问题1

使用皮尔逊相关系数分析，评分与要求AI助手总结文本的消息数量（𝑟=0.310，𝑝<0.1）及总编辑次数（𝑟=0.397，𝑝<0.05）呈正相关。这些相关性表明，人类在协作过程中更积极的参与可能会提升最终输出质量。

两位研究者将AI助手的回复标注为四类：成功回应用户提示、脱离上下文、未能回应、拒绝回应。后三类被归为失败回应。多项变量与AI回复成功与否显著相关。

皮尔逊相关系数显示，当参与者采用复制（𝑟=0.581，𝑝<0.01）或转述问题（𝑟=0.522，𝑝<0.01）的方式提问时，更可能获得成功回应。

此外，成功回应数量与询问AI功能范围的问题数量正相关（𝑟=0.693，𝑝<0.01）。

但AI常无法理解过度转述的提示，例如未能理解"这个问题的最佳解决方案是什么？"，因其使用"problem"而非最初输入AI模型时使用的"question"一词。

#### 问题2

研究表明，在"Nudging"（助推）实验条件下，查询快捷功能显著改善了整体交互效果。数据显示，消息建议切实影响并改变了参与者的行为模式。

例如，在第三种实验条件下，要求AI转述参考文档的用户比例明显高于其他条件。

通过Tukey HSD检验发现，在提供建议的"Nudging"条件下，用户要求AI总结文本的消息数量显著多于"Highlight"条件（p < 0.01）。

同样，请求AI转述文本的消息量在"Nudging"条件下也显著高于"Highlight"（p < 0.01）和"Conversation"条件（p < 0.01）。

此外，询问AI功能用途的消息频率在"Nudging"条件下也明显更高（相较于"Highlight"和"Conversation"条件，p值均<0.01）。

总体而言，"Nudging"条件下用户发送给AI助手的消息总量显著多于"Highlight"条件（p < 0.01）。

这些行为变化在人机协作层面产生了积极影响，促成了更成功的交互。

与"Highlight"（p < 0.01）和"Conversation"条件（p < 0.1）相比，"Nudging"条件下AI助手的成功响应率更高。皮

尔逊相关系数显示，消息建议的点击总量与AI成功响应次数呈显著正相关（r=0.626，p<0.01）。

在问卷调查中，有参与者评价"Nudging"条件："这帮助我理解了AI的功能及其最佳使用方式"；另一参与者指出："它提供了很好的切入点"。这些反馈表明，该设计有效提升了用户对AI能力的认知，并优化了辅助功能的使用引导。

#### 问题3 

最常被提及的是准确性，涉及核对响应事实是否与参考文档一致，以及实际解决问题的有效性。

其次是响应语气，考察是否友好自然、拟人化。

第三大标准是响应长度，关于长短优劣的争议详见8.3节。参与者还提到了清晰度、相关性和语法正确性。

最后，部分评分者关注响应内容的伦理合规性，这与表1中涉及手机越狱的最终问题相关。


## Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions

### introduction

在多Agent系统兴起与虚拟Agent日益普及的背景下，一个有趣的问题浮现：一组AI Agent能否对用户产生集体社会影响力？这个问题源于人际互动中的经典发现：个体更容易受到多数人观点的影响，这种社会影响现象表现为从众行为[24]、同辈压力[47,88]、社会认同渴望[68,80]和社会规范遵从[37,63]等多种形式。

问题：持相同观点的一组AIAgent能否以类似人类群体的方式影响用户决策？

该问题的重要性体现在两方面：其一，多Agent交互日益普遍，在X（原Twitter）等社交平台上，类人社交机器人已被有效用于信息传播和用户参与[110]。
多Agent系统还支持在线讨论[118]、公共信息收集[51]和健康指导[9]，使用户在多场景接触多个Agent的观点。其二，多Agent系统潜在的社会影响力令人担忧。

人机交互领域研究进一步支持了AI智能体产生说服效果的潜力，这些研究强调单个AI智能体对用户态度[49]和行为[89,116]产生的显著社会影响。然而此类研究主要关注单个智能体的设计特征（如被感知为聊天机器人或人类对结果的影响[89]），鲜少探讨智能体数量如何影响系统有效性。相比之下，多智能体系统研究主要聚焦于提升任务绩效[19,42]，无论是帮助用户更高效完成任务还是提升系统技术性能[18,32].

当系统主要目标是明确智能体在讨论中的角色或提升其技术性能时，社会影响的作用较不显著。但在需要同时传递信息并说服用户接受特定观点的场景中，多智能体与单智能体的效果差异亟待探索。

鉴于：1）多智能体系统及与多个社交机器人交互日益普遍；2）AI智能体已知的社会影响力；3）人类群体社会影响的既定效应可能在人机交互中复现；4）AI智能体群体产生的集体社会影响具有显著正负效应潜力。本研究通过以下问题探讨该议题：与多个虚拟智能体交互能否产生改变我们观点的社会影响？具体而言，我们探究多智能体交互是否会导致更强烈的观点改变、引发差异化的社会影响，并识别增加AI影响易感性的群体特征，旨在为多智能体系统制定伦理准则。

定量与定性分析表明：多智能体设置显著影响观点转变。当智能体持反对意见时，受试者表现出更强烈的观点改变；当智能体赞同时则出现更严重的观点极化。但智能体数量从3增至5时，改变强度并未提升——5智能体组反而引发极化现象，更多受试者拒绝智能体论点并远离其立场。此外，多智能体组强化了社会影响感知，尤其是与多数保持一致的标准压力。开放式回答显示受试者在3/5智能体条件下表现出群体归属需求，这促进了社会影响与观点转变。人口统计分析揭示年轻群体更易受多智能体影响。

研究为人机交互（HCI）和计算机支持的协同工作（CSCW）领域的未来研究做出了以下贡献：

1. 我们证明，在讨论中引入多个智能体更容易产生社会影响力并推动观点转变。这填补了关于多智能体系统如何影响人类认知的研究空白，揭示了相较于单智能体系统，多智能体配置能引发更强烈的态度转变。
2. 我们深化了对单智能体与多智能体系统社会影响力差异的理解。通过观察参与者的社会排斥感和参与意愿，我们发现了人类对智能体群体的隐性归属感，从而将现有社会影响理论拓展至人机交互领域。
3. 我们为利用多智能体配置更有效实施说服策略提供了设计建议。通过发挥多智能体产生的规范压力，未来系统可以增强参与者的情感投入，从而促成更持久、更内化的态度与行为改变。

### 研究背景(仅摘取其中有关agent的片段)

AI Agent正日益融入我们的日常生活。这些先进的AI系统能够与用户互动、进行对话并表达自身观点[107]。近期研究和行业应用利用大语言模型（LLMs）的对话推理能力，开发出问题解决工具[40,67,82,108,115]、教育助手[59]和新型搜索界面[62,113]。商业领域中，企业通过社交媒体部署AIAgent进行品牌营销和客户服务[46,50,112]，或将Agent嵌入应用程序辅助写作[31]、头脑风暴[78]和游戏[90]。

然而，基于单一LLM的代理仍难以有效处理诸多任务。独立LLM可能缺乏专业领域知识[35]，推理能力也尚不完善。受心理学理论[92]启发，研究者探索了多代理"辩论"[32]与"调解"[18]等技术，通过观点碰撞提升推理质量。这推动了"多代理协作"研究热潮——通过智能体协同提升任务表现[19,42]。自然语言处理[42]、软件工程[14]和机器人学[20]等领域已运用多代理集体智慧模拟群体动态，优化任务执行[45,76,111]。多代理系统还能模拟人类角色互动，如工作者协作[4,60]或多人交互[72,73]。

在用户中心场景中，同步多代理交互成为新趋势。例如Clarke等人[25]开发了可同时调用Google助手、SoundHound和福特车载助手Adasa的集成界面。Anthropic[26]、谷歌[38]等企业也推出多代理框架，方便开发者部署多代理系统。部分机构更在社交媒体采用多机器人矩阵与用户互动。

现有工作多聚焦通过多代理系统降低信息获取认知负荷[51]，或整合多元信息辅助决策[9,16,71,100]，主要探讨用户面对不同输入时的困惑感[16,71,100]。迄今尚无研究探讨代理群体是否/如何通过社会影响机制作用于用户——这种区别于认知决策的影响机制可能同样重要。

多余内容与问题无关。

## LLM-Based Test-Driven Interactive Code Generation: User Study and Empirical Evaluation

### Introduction

直接代码生成可能存在的问题：

- 因提示词不明确而导致的意图模糊
- 无法确定由自然语言提示词驱动生成的代码是否正确

多项关于开发者与AI交互的研究[9][14]强调，需要建立机制来辅助验证AI生成代码的正确性，例如允许用户通过测试来区分不同代码建议[15]的方案。

本文提出了测试驱动的交互式代码生成（TICODER）的工作流程，以（a）通过生成的测试用例澄清（即部分形式化）用户意图，以及（b）生成与这些测试用例一致的代码排名列表。

并且在基于大语言模型（LLM）的代码生成场景中，自动生成的测试不仅能帮助明确自然语言意图并筛除错误建议，还可作为剩余建议的调试辅助工具及未来代码修改的回归测试[7]。

此外，交互框架的有效与否取决于：(a)LLM生成有效测试的能力，(b)用户交互开销与代码建议筛选排序收益间的权衡。

为此,文章探究：
该工作流如何影响开发者评估AI生成代码的性能？同时该框架应具备扩展性，能提升开源/闭源LLM的代码生成准确率。因此我们还研究：
该工作流能否提升代码生成模型的准确率？

通过(1)混合效应用户研究及(2)两个Python代码生成基准的大规模评估，我们验证了框架有效性。本文贡献如下：

1. 提出TI CODER交互工作流，通过自动生成测试引导用户澄清意图，提升LLM代码生成准确率。该系统利用现成LLM生成代码与测试，并通过用户认可的测试验证AI生成代码。
2. 通过混合方法用户研究对比TI CODER两种变体（含代表现有开发者-AI交互流程的基线条件），发现使用任一变体均能显著降低认知负荷。
3. 通过模拟用户反馈（以参考答案为理想Agent）大规模评估TI CODER性能。在MBPP和HumanEval数据集及四种开源/闭源LLM上的实验表明：该系统能在5次交互内使pass@1准确率平均提升45.73%，仅需1次交互即可让小模型达到GPT-4-32k级别的大模型性能。

### 研究背景

#### 提高代码生成准确度

提到类似的通过生成测试用例来提高准确度的工作,不过类似的工作在交互性上不比本工作好,因此未必能提高用户的满意度。另外的一些研究依赖自动符号引擎（如约束求解器[30]或自动机构造[17]）为程序对生成差异化输入样例，这对Python等通用命令式编程语言并不适用。

#### AI编程助手的可用性

现有研究对AI编程助手的可用性进行了多维度探索。这里重点讨论与意图表达和生成建议控制相关的挑战。Liang等人[8]发现"放弃整合生成代码"和"无法提供反馈"是基于补全的AI编程助手最常见的使用障碍，这通常源于代码未实现预期功能、用户不理解生成逻辑或难以控制输出与意图对齐。McNutt等人[31]构建了代码助手交互设计空间，包括消除候选程序歧义和优化初始规约的方案——这与GitHub Copilot[32]等助手和传统程序合成工具[33]中歧义消除的价值研究相呼应。Xu等人[9]探究了IDE集成AI助手的挑战，包括用户查询的明确程度，发现自然语言查询中经常存在意图表达困难，而规约不足问题多源于变量名省略等模糊指令。

另一方面,Bird等人[14]的最新研究表明，AI工具正使开发者角色发生转变——**代码审查时间已超过实际编写时间**。

### 研究问题

RQ1 TiCoder如何影响Python开发者评估AI生成代码的性能，包括任务正确性、时间和认知负荷？为回答RQ1，我们进行了用户研究，参与者使用结合TiCoder工作流程的AI助手。我们评估了所提方法在开发者评估AI生成代码时的成本效益权衡。

RQ2 TiCoder工作流程是否能提高生成代码建议的准确性？为回答RQ2，我们探索了结合TiCoder工作流程的大语言模型在两个Python代码生成基准测试中的代码生成准确性。

### 交互流程

TiCoder 的交互流程如下：

用户输入：提供函数头（function header）、自然语言描述（NL prompt）以及可选的上下文代码。

模型生成候选：LLM 生成一组候选代码（code suggestions）和一组候选测试（test suggestions）。

执行测试：对每条候选代码，执行所有候选测试，记录哪些测试通过/失败/崩溃。

测试排序：根据“区分能力”对测试进行排序，优先向用户展示最能区分不同代码行为的测试。

用户反馈：用户对测试做出回应：

- PASS：该测试输出符合预期；
- FAIL：不符合预期；
- UNDEFINED：输入违反函数前提条件（如除零、空列表等）；

（在 TICODER-OUTPUT 模式下）若为 FAIL，用户还需提供正确的输出值。

剪枝与重排序：根据用户反馈，剔除与用户意图冲突的代码，并对剩余代码按通过测试数量重新排序。

重复交互：可进行多轮（论文中最多5轮），逐步缩小候选空间。

最终输出：返回用户确认的测试集 + 与之行为一致的排序后代码列表。

设计上提出两种交互变体：

TICODER-PASSFAIL：仅回答 PASS / FAIL / UNDEFINED。

TICODER-OUTPUT：FAIL 时需提供正确输出值。

### RQ1实验结果

- TiCoder如何影响任务正确性？

从机制设计来看，当用户错误评估测试用例时，TiCoder工作流会剪裁所有通过该测试的有效程序。因此错误评估测试的参与者将看不到任何有效的AI生成程序，除非他们声明所有代码建议都不正确。在TiCoder工作流中，用户的噪声响应会确保生成代码不符合其真实意图。因此实践中，跳过测试评估的选项对工作流可用性至关重要，能减少用户噪声输入带来的不确定性。尽管TiCoder能有效辅助用户评估代码建议，但噪声反馈的风险仍需警惕。

- TiCoder如何影响耗时？

结果：TiCoder引入的测试验证时间不会显著增加总耗时。

- TiCoder如何影响认知负荷？

作者认为这种效果的出现可能是因为用户需要评估的代码建议数量减少了，而且测试用例可以作为理解代码的具体机制，同时为更全面地测试候选函数提供了一个起点，从而更容易开始任务。

### RQ2实验结果

就反正是能。

### 讨论和结论

聚焦于代码生成鲁棒性和通用性和模型方面的考虑。无实际交互方面的考量。